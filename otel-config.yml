receivers:
  prometheus:
    config:
      scrape_configs:
        - job_name: "prosody"
          scrape_interval: 120s
          static_configs:
            - targets: ["prosody:5280"]
          metrics_path: "/metrics"

        - job_name: "jicofo"
          scrape_interval: 120s
          static_configs:
            - targets: ["jicofo:8888"]
          metrics_path: "/metrics"

        - job_name: "jvb"
          scrape_interval: 120s
          static_configs:
            - targets: ["jvb:8080"]
          metrics_path: "/metrics"

        - job_name: "keycloak"
          scrape_interval: 180s
          static_configs:
            - targets: ["keycloak:9000"]
          metrics_path: "/auth/metrics"

  # System-level metrics (Ubuntu host metrics)
  hostmetrics:
    collection_interval: 120s
    scrapers:
      cpu: {}
      memory: {}
      disk: {}
      network: {}
      load: {}
      filesystem: {}

  # Docker container stats for automatic container metadata
  docker_stats:
    endpoint: unix:///var/run/docker.sock
    collection_interval: 120s
    timeout: 5s
    api_version: "1.44"

  filelog:
    include:
      - /var/lib/docker/containers/*/*-json.log
    include_file_path: true
    start_at: end
    poll_interval: 5s
    operators:
      # Parse Docker JSON log format
      - type: json_parser
        parse_from: body
        parse_to: attributes
      # Extract container ID from file path
      - type: regex_parser
        regex: "/var/lib/docker/containers/(?P<container_id>[^/]+)/"
        parse_from: attributes["log.file.path"]
        parse_to: resource
      # Store original log message
      - type: move
        from: attributes.log
        to: body
      # Parse timestamp from Docker log and use it as the main timestamp
      - type: time_parser
        parse_from: attributes.time
        layout: "%Y-%m-%dT%H:%M:%S.%fZ"
      # Remove the separate 'time' attribute since we're using it as the main timestamp
      - type: remove
        field: attributes.time
      # Store stream info as attribute
      - type: add
        field: attributes.stream
        value: EXPR(attributes.stream)
      # Remove the obscure log file path, we'll add a cleaner one later
      - type: remove
        field: attributes["log.file.path"]

processors:
  batch: {}

  resourcedetection:
    detectors: ["env", "system", "docker"]
    override: false
    timeout: 30s

  # Resource processor to set server configuration
  resource:
    attributes:
      - key: host.name
        value: "${env:SERVER_NAME:-unknown}"
        action: upsert
      - key: deployment.environment
        value: "${env:ENVIRONMENT:-production}"
        action: upsert
      - key: deployment.region
        value: "${env:REGION:-paris-1}"
        action: upsert

  # Transform processor for metrics - minimal cleanup
  transform/process_metrics:
    error_mode: ignore
    metric_statements:
      - context: resource
        statements:
          # Clean up unnecessary resource attributes
          - delete_key(resource.attributes, "os.description")
          - delete_key(resource.attributes, "process.command_args")
          - delete_key(resource.attributes, "process.executable.path")
          - delete_key(resource.attributes, "process.pid")
          - delete_key(resource.attributes, "process.runtime.description")
          - delete_key(resource.attributes, "process.runtime.name")
          - delete_key(resource.attributes, "process.runtime.version")

  # Transform processor for logs - set proper service names and clean up attributes
  transform/process_logs:
    error_mode: ignore
    log_statements:
      - context: resource
        statements:
          # Set service name based on container name (more useful than hostname)
          - set(resource.attributes["service.name"], resource.attributes["container.name"]) where resource.attributes["container.name"] != nil
          # Fallback to a meaningful service name if container name is not available
          - set(resource.attributes["service.name"], "docker-container") where resource.attributes["service.name"] == nil or resource.attributes["service.name"] == "unknown_service"
          # Set host name to server name from environment
          - set(resource.attributes["host.name"], "${env:SERVER_NAME}") where resource.attributes["host.name"] == nil
          # Set deployment environment and region
          - set(resource.attributes["deployment.environment"], "${env:ENVIRONMENT:-production}")
          - set(resource.attributes["deployment.region"], "${env:REGION:-us-east-1}")
          # Create a human-readable container identifier
          - set(resource.attributes["container.short_id"], Substring(resource.attributes["container.id"], 0, 12)) where resource.attributes["container.id"] != nil
          # Keep essential resource attributes and remove clutter
          - keep_keys(resource.attributes, [
            "deployment.environment",
            "deployment.region", 
            "service.name",
            "host.name",
            "container.name",
            "container.short_id",
            "container.image.name"
            ])
      - context: log
        statements:
          # Set severity number based on common log levels found in body
          - set(severity_text, "DEBUG") where IsMatch(body, "(?i)debug")
          - set(severity_text, "INFO") where IsMatch(body, "(?i)info")
          - set(severity_text, "WARN") where IsMatch(body, "(?i)warn")
          - set(severity_text, "ERROR") where IsMatch(body, "(?i)error")
          - set(severity_text, "FATAL") where IsMatch(body, "(?i)fatal")

          # Set severity number based on text level
          - set(severity_number, 5) where severity_text == "DEBUG"
          - set(severity_number, 9) where severity_text == "INFO"
          - set(severity_number, 13) where severity_text == "WARN"
          - set(severity_number, 17) where severity_text == "ERROR"
          - set(severity_number, 21) where severity_text == "FATAL"
          
          # Remove redundant attributes that are now handled better
          - delete_key(attributes, "log.file.name")
          - delete_key(attributes, "log.file.path")

exporters:
  otlphttp/grafana_cloud:
    endpoint: "https://otlp-gateway-prod-eu-west-2.grafana.net/otlp"
    auth:
      authenticator: basicauth/grafana_cloud

extensions:
  basicauth/grafana_cloud:
    client_auth:
      username: "${GRAFANA_USERNAME}"
      password: "${GRAFANA_API_KEY}"

service:
  extensions: [basicauth/grafana_cloud]
  telemetry:
    metrics:
      level: none
  pipelines:
    metrics:
      receivers: [prometheus, docker_stats, hostmetrics]
      processors:
        - resourcedetection
        - resource
        - transform/process_metrics
        - batch
      exporters: [otlphttp/grafana_cloud]
    logs:
      receivers: [filelog]
      processors:
        - resourcedetection
        - resource
        - transform/process_logs
        - batch
      exporters: [otlphttp/grafana_cloud]
